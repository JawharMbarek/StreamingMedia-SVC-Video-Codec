\input{header}

%Bibtexformat
%\usepackage[style=authortitle-icomp, backend=bibtex]{biblatex}
\usepackage[style=numeric, backend=bibtex]{biblatex}
%Link zur bib
\bibliography{literatur/bib}  
%zeilenabstand Anhang
\setlength{\bibitemsep}{1em} 
%http://biblatex.dominik-wassenhoven.de/download/DTK-2_2008-biblatex-Teil1.pdf
% Beispiel footcite
%\footcite[Vgl.][Seite 5]{Bal.2009}

\begin{document}
\pagenumbering{Roman}

\include{deckblatt}

\tableofcontents
\setcounter{page}{1}

\chapter{Einleitung}

\pagenumbering{arabic}

In den letzten Jahren hat sich eine große Wandlung im Broadcastingumfeld ergeben. Während früher die
Auslieferung von Videocontent in Standardauflösung über DVB-S und DVB-T in MPEG2 nahezu die einzige
Art der Videodistribution war, ist der Distributionsprozess heute viel stärker diversifiziert und interaktiv. 

Potentielle Kunden erwarten heutzutage die parallele Auslieferung des Videocontents in regulärer SD- als auch
in FullHD-Auflösung. Weiterhin sind die Empfangsgeräte der Kunden sehr viel inhomogener in Bezug auf 
technische Anbindung als auch Dekodierfähigkeiten. Videoinhalte werden heutzutage sowohl stationär auf regulären
Fernsehegeräten und modernen SmartTVs als auch mobil auf Tabletts und Smartphones konsumiert. Dabei gewinnt 
gleichzeitig auch die Interaktivität durch zusätzliche Medieninhalten im Rahmen des hybriden Fernsehen an Bedeutung.
Dadurch stehen die traditiononellen Anbieter dieser Dienste vor der großen Herausforderungen, 
wie sie ihre Inhalte an die Kunden über verschiedene Kanäle in verschiedenen Auflösungen gleichzeitig übertragen 
könnnen. Mit Scalable Video Coding (SVC) steht eine Technologie zur Verfügung, die dieses zu Leisten im Stande ist.
    
In dieser Ausarbeitung wird Scalable Video Coding detailliert vorgestellt. Im ersten Teil der Arbeit wird vorgestellt 
welche aktuellen Herausforderungen sich im Broadcastinbereich stellen. Im zweiten Teil wird die Scalable Video 
Coding Extension des H264/AVC-Standards vorgestellt. In diesem Abschnitt wird zuerst ein allgemeiner Ausblick 
auf die Notwendigkeit und Vorteile von SVC gegeben. Danach wird die Videokodierung mit H264/AVC vorgestellt 
und anschließend die Neuerungen zwischen H264/AVC und H264-SVC gegenübergestellt. Im nächsten Abschnitt wird 
die Verwendung von Scalable Video Coding im Rahmen von MPEG DASH vorgestellt. Im nächsten XXXX 

Im letzten Abschnitt wird abschließend eine Zusamenfassung über den Themenbereich Scalable Video Coding gegeben.

\chapter{Aktuelle Trends und Herausforderungen}
In den vergangenen Jahren dominierten verschiedene Themen das Broadcastingumfeld. Im Jahr 2009
befand sich die Broadcastingindustrie inmitten der Vorbereitung auf die Umstellung auf HD. Daher
wurde dies als wichigster Trend der nächsten Jahre gesehen\parencite[1]{Zaller.2012.Trends}. 
In Deutschland begann im Jahr 2010 beispielsweise die Ausstrahlung von ARD HD im regulärem Sendebetrieb.

In den Jahren 2010 und 2011 wurde die Distribution über mehre Plattformen, das sogenannte
"`Multiplatform delivery"', als wichtigster Trend gesehen\parencite[1]{Zaller.2012.Trends}.
Auch im Jahr 2012 liegt dies mit weitem Abstand vor anderen Trends weiterhin vorn. Einhergehend damit
wurden als weitere wichtige Trends die Opimierung des Workflows und die Übertragung via IP genannt
\parencite[1]{Zaller.2012.2012}. 
Die Ausrichtung auf die Bedienung mehrere Distributionskanäle und Plattformen hat mehrere Gründe.

Einerseits haben sich die Sehgewohnheiten der Konsumenten geändert. Beispielsweise sank im Jahr 2011 die
Zahl an amerikanischen Haushalten, welche keinen Fernseher besaßen, zum ersten Mal in 20
Jahren\parencite[1]{Stelter.2011}. Das bedeutet nicht automatisch, dass der Fernsehkonsum rückläufig
ist und durch das Internet ersetzt wird, sondern dass es zu einer Medienkonvergenz kommt.
Traditionelles Fernsehen und Internet verschmelzen. Fernsehen wird über das Internet geschaut und
über Fernsehgeräte bzw. SmartTVs wird im Internet gesurft und Apps werden ausgeführt 
\parencite[180]{Hohlfeld.2010}.

Bereits im Jahr 2011 bestand über die Hälfe des ganzen Internetverkehrs der Konsumenten aus Videodaten. Bis 
zum Jahr 2016 wird sich der Wert auf ca. 56\% steigern. Der Anteil an Videodaten für SmartTVs lag im Jahr 
2011 bei 8\% und wird bis zum Jahr 2016 auf 12\% steigern\parencite{Cisco.2012}

Die Möglichkeit Webinhalte auf dem Fernsehgerät bzw. SmartTV zu empfangen, ermöglicht einerseits neuen 
Konkurrenten den Einstieg in den Mark. Sogenannter Over-the-top-Content, also im Internet frei verfügbare 
und durch Werung finanzierte Inhalte, wie beispielsweise Youtube oder Hulu im amerikanischen Markt, können auf
lange Sicht gegenüber etablierten Anbietern im Broadcastingumfeld Marktanteile
gewinnen\parencite[2]{Sader.2008}.

Andererseits bieten Fernsehgerät mit Internetzugang auch für etablierte Anbietern im Broadcastingumfeld 
eine neue Einnahmequelle im Rahmen von HybridTV. Zusätzlich bietet es dem Zuschauer durch Zusatzinformationen
einen Mehrwert und kann sich so positiv auf die Zuschauerzufriedenheit und Marktanteile
auswirken\parencite{techfacts.2011}

Weiterhin nimmt die Anzahl unterschiedlichen Endgeräte mit heterogenen Dekodierfähgikeiten stark
zu. Während Broadcastinganbieter früher im Rahmen von Ausstrahlungen in DVB-S oder DVB-T davon ausgehen konnte, 
dass nahezu alle Geräte die in MPEG2 ausgestrahlten Inhalte bis zu einer bestimmten Bitrate dekodieren können, 
gibt es heute einerseits Geräte am unteren Ende der Skala, die nur SD-Content im H264/AVC-Baseline Profile 
abspielen können, wie beispielsweise ältere Android Sartphones. Anderereits gibt es wiederum Geräte, die 
FullHD in  H264/AVC-High Profile dekodieren können, z.\,B. reguläre Setupboxen und 
SmartTVs\parencite[3]{Sader.2008}.

Ein weiterer Grund für die Forcierung auf mehrere Distributionskanäle ist der heterogene
Übertragungskanal zwischen Sender und Empfänger\parencite[4]{Sader.2008}. Die Empfänger
sind sowohl mit sehr hoher Bandbreite von teilweise 100Mbit/s angebunden, als auch mit einer relativ
geringen Bandbreite von unter 1Mbit/s.
Durschnittlich beträgt die Bandbreite in Deutschland derzeit beispielsweise nur 5,8 Mbit/s und wird in den nächsten
Jahren nur langsam steigen\parencite[1]{Schieb.2012}.
Erschwerend kommt hinzu, dass im mobilen Bereich durch die Shared-Media-Charakteristik die
Bandbreite stark schwanken kann.

Um diesen sich in Zukunft weiter wachsenden Herausforderungen zu begegnen, bieten sich skalierbare Videocodecs
an. Diese sollen im folgenden näher vorgestellt werden.

\chapter{H264-SVC in a Nutshell}

\section{Allgemein}
Im Bezug auf einen Videostrom versteht man unter dem Begriff "`Skalierbarkeit"' die Eigenschaft, dass Teile des
Videostroms entfernt werden können und der Videostrom nach Entfernen weiterhin valide und abspielbar
bleibt. Dieser Teilvideostrom bildet den Originalvideostrom mit verringerter Qualität ab.
Videoströme, die diese Fähigkeit nicht besitzen, nennt man Singlelayer-
Videoströme\parencite[2]{SchwarzNeu.2007}.

Skalierbare Videocodecs befinden sich seit nunmehr 20 Jahren in der Entwicklung. Bereits ältere
Videocodecs wie das angesprochene MPEG2 boten bereits durch standardisierte Erweiterungens (z.\,B. MPEG2 Video) die
Möglichkeiten zur Übertragung von skalierbaren Videoströmen \parencite[1]{SchwarzNeu.2007}.

Allerdings wurden diese Codecerweiterungen nur selten benutzt, da der Bedarf nach Auslieferung von
skalierbaren Videoinhalten noch vergleichweise gering war. Dies ist durch die relativ homogene 
technische Ausstattung der Empfänger und deren Anbindung an das Internet zu erklären.
Zusätzlich verringerte sich durch den Einsatz der SVC-Erweiterungen in älteren Codecs die Effizienz der 
Videokodierung stark, während gleichzeitigem Rechenaufwand anstieg \parencite[1f]{Shahid.2010}.

In den vergangenen Jahren hat in Bezug auf die technisch Anbindung und Ausstattung der Empfänger
eine starke Heterogenisierung eingesetzt, sodass skalierbare Videokodierung und Distribution immer 
mehr in den Fokus rückt. Weiterhin hat sich mit Verabschiedung von H264/AVC im Jahr 
2007 \parencite[2]{Shahid.2010} ein Codec etabliert, der sehr viel effizienter kodiert als ältere 
Videocodecs\parencite[2]{Shapiro.2009}.
 
Entwickelt vom JVT\footnote{JVT - Joint Video Team} und auf dem regulären H264/AVC-Standard aufbauend, ist
H264-SVC ist eine Erweiterung, die als Amendment 3 in Annex G in dieses Standard aufgenommen
wurde\parencite[10]{Shapiro.2009}.
Damit benutzt H264-SVC die effizienten Enkodierfunktionen und technischen Features des
H264/AVC-Videostandards und erweitert diesen, um den zu übertragenen Videostrom skalierbar zu
machen\parencite[59]{Cho.2010}.
Zusätzlich bleibt dadurch die Rückwärtskompatibilität zu herkömlichen H264/AVC-Dekodern gewahrt.
\parencite[601]{Sader.2008}

H264-SVC bietet im Broadcastingumfeld einige Vorteile gegenüber anderen derzeit eingesetzten Verfahren, 
wie beispielse Simulcast, bei welchem jeder Videostrom unabhängig voneinander kodiert und übertragen wird.
\parencite[1]{Shahid.2010} Auf H264/AVC aufbauend, ist H264-SVC vollständig abwärtskompatibel zu 
herkömlichen H264-Dekodern. 

\begin{figure}[htb]
	\vspace{0.5cm}
	\centering
	\includegraphics[width=0.95\textwidth]{bilder/typesOfLayers.jpg}
	\caption[Arten der Skalierbarkeit]{Arten der Skalierbarkeit (Eigene Abbildung nach \parencite[143]{Richardson.2008})}
		%Eigene Abbildung nach  H.264 and MPEG-4 Video Compression Seite, 2008, Seite 143}	
	\label{fig:typeofenhancementlayers}
	\vspace{0.5cm}
\end{figure}

Diese ignorieren die SVC-Erweiterung und dekodieren nur die regulären H264/AVC-Inhalt.
Dies ist möglich indem neben einem regulärem H264/AVC-Videostrom, dem sogenannten Baselayer, weitere
Videoströme, den sogenannten Enhancementlayers, eingebettet werden.
Diese Enhancementlayers können sich gegenüber dem Baselayer in Bezug auf Framerate (Temporal),
Auflösung (Spatial) und Bildqualität (Quality) unterschieden (Siehe Abbildung
\ref{fig:typeofenhancementlayers}) \parencite[601]{Sader.2008}.
In skalierbaren Videoströmen können diese drei Arten von Enhancementlayern auch beliebig kombiniert werden,
z.\,B. Spatio–temporale Enhancementlayer \parencite[2]{SchwarzNeu.2007}.

%http://www.isccast.com/podcasts.isc365.com/powerpoint/09ISCWest_DI08.pdf

Zusätzlich können mit einer in H264-SVC enkodierten Videoquelldatei gleichzeitig unterschiedliche Geräte z.\,B. 
Smartphones und FullHD-Geräte angesprochen werden. Die sonst übliche Transkodierung und das Vorhaltung 
vorab enkodierter Teilvideodatein entfällt. Das erleichtert die Langzeitarchivierung und verringert gleichzeitig
den benötigten Speicherplatz.

Bei Verwendung dieser H264-SVC erhöht sich der Overhead nur um 10\% bis 20\% verglichen mit regulärem
H264/AVC. Im Falle eines stark fehleranfälligen Übertragungsmediums kann der Overhead sogar geringer ausfallen,
als bei Simulcastübertragung. Zusätzlich bleibt der zur Enkodierung und Dekodierung benötigte Rechenaufwand 
nahezu gleich \parencite[4]{Shapiro.2009}.

\section{Abgrenzung zu Adaptive Streaming}

\section{Videokodierung mit H264/AVC}
Als Erweiterung des H264/AVC-Standards benutzt H264-SVC den gleichen Enkodierablauf und erweiterten diesen
in wenigen Bereichen, um Skalierbarkeit zu ermöglichen. Die Videkodierung mit H264/AVC erfolgt in vier 
Teilschritten. Zuerst wird der aktuelle Videoframe (\textit{F}) in mehrere Slices unterteilt, die wiederum 
Makroblöcke enthalten \parencite[146]{Hottong.2009}. 
In jedem Makroblock sind die Luminanz und Chrominazwerte gespeichert. Aufgrund des Chromasubsamplings liegen 
die Luminanzwerte in den meisten Fällen nur mit halbierter Auflösung vor.

Die Größe der Makroblöcke ist auf 16x 16 Pixel festgelegt. Makroblöcke können wiederum in verschiedene 
Makroblockpartitionen unterteilt werden z.B  8 x 8 Pixel Blöcke. Diese Makroblockpartitionen können selbst 
wieder in Sub-Makroblockpartitionen von beispielsweise 4 x 4 Pixel unterteilt werden. Diese so entstehende 
Unterteilung wird als \textit{Tree Structured Motion Compensation} 
bezeichnet \parencite[160]{Richardson.2008}.

Neben der Makroblockgröße unterscheidet man auch den Typ eines Makroblocks und damit die Art der Slices. 
I-Makroblöcke werden von vorherigen Makroblöcken des selben Slices kodiert und dienen anderen Makroblöcke als 
Referenz. P-Makroblöcke referenzieren auf vorhergegangene I-Makroblöcke von vorherigen Frames und
ermöglichen dadurch eine Steigerung der Kompressionrate. B-Makroblöcke referenzieren sowohl auf 
vorhergegangene- als auch auf nachfolgende I-Makroblöcken dieser Frames und ermöglichen so eine weitere 
Steigerung der Kompression \parencite[164f]{Richardson.2008}.
 
Analog zu den Makroblöcken werden auch die Slices nach I, P und B-Slice unterschieden. I-Slices enthalten 
nur I-Makroblöcke, während P-Slices sowohl I-Makroblöcke als auch P-Makroblöcke enthalten können. B-Slices 
können I-Makroblöcke und B-Makroblöcke enthalten \parencite[159f]{Richardson.2008}. 
%Zur effizienteren Verwaltung der Referenzbilder bei Verwendung von B-Slices werden diese in einen Index eingetragen.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{bilder/h264codec.pdf}
	%\caption[H264/AVC-Enkoder \newline \textit{Quelle: H.264 and MPEG-4 Video Compression, 2008}]{H264/AVC-Enkoder}
	\caption[Funktionsweise eines H264/AVC-Enkoders]{Funktionsweise eines H264/AVC-Enkoders 
	\parencite{Richardson.2008}}
	\label{fig:h264enkoder}
\end{figure}

Bei der Kodierung eines Frames (\textit{F}) wird für jeden Makroblock eine Voraussage getroffen.
Diese kann eine intra-basierte Voraussage sein, die auf der Kodierung von vorangegangener Makroblöcken des
gleichen Slice beruht. Alternativ kann es auch eine intra-basierte Voraussage sein, die auf der 
Referenzierung und Kodierung von Makroblöcken aus vorausgegangenen- und nachfolgenden Frames beruht. 
In beiden Fällen wird die Differenz zwischen dem aktuellen Makroblock und der Voraussage gebildet und 
die Abweichungen zwischen beiden Makroblöcken gespeichert\parencite[161f]{Richardson.2008}.

Im nächsten Schritt werden diese Abweichungen quantisiert und die Koeffizienten in einem Zickzack-Verfahren 
angeordnet\parencite[198f]{Richardson.2008}. Dieses Ausleseverfahren ist für die anschliessende 
Entropiekodierung besser geeignet, da so ähnliche Koeffizienten nacheinander stehen und diese somit 
einfacher zusammenfassbar sind \parencite[30]{Hottong.2009}.
Bei H264/AVC kommen mehrere Verfahren zur Entropiekodierung zum Einsatz. Neben Variable-Length Codes (VLC) 
auch Context-Adaptive Arithmetic Coding (CABAC), Context-Adaptive Variable Coding (CAVLC) 
und Exp-Golomb \parencite[198f]{Richardson.2008}.

Im letzten Schritt werden die entropiekodierten Koeffizienten zu einem Videostrom zusammengefasst und 
durch den Network Abstraction Layer (NAL) übertragen\parencite[161f]{Richardson.2008}

\section{Videokodierung mit H264-SVC}

\subsection{Baselayer und Enhacementlayer}
Zur Erzeugung von skalierbaren Videoströmen werden H264-SVC Videoströme in zwei Arten von Layer unterteilt.
Einereits gibt es den sogenannten Baselayer, welcher in jedem H264-SVC Videostrom vorhanden sein muß.
Zusätzlich gibt es auf diesen aufbauend mehrere Enhacementlayer, die Videoströme mit
unterschiedlichen Qualitätsstufen (Auflösung, Framerate und Qualität) enthalten.\parencite[1]{Unanue.2011}

\begin{figure}[hb]
	\centering
	\includegraphics[width=0.95\textwidth]{bilder/svc_layer2}
	%\caption[h264enkoder]{Base- und Enhancementlayer 
	%\\ textit{Quelle: H.264 and MPEG-4 Video Compression, Seite 142, 2008}}
	\caption[Base- und Enhancementlayer]{Base- und Enhancementlayer \parencite[142]{Richardson.2008}}
	\label{fig:h264enkoder}
\end{figure}

Der Baselayer wird im regulärem H264/AVC-Modus enkodiert. Die Kodierung erfolgt mit festgelegter 
Group of Pictures bzw. GOP und freier Refrenzierung der einzelnen Frames innherhalb jeder GOP.
Dadurch kann der Baselayer auch von regulären, nicht H264-SVC-fähigen H264/AVC Videodecodern dekodiert- 
und angezeigt werden. Bei diesen Geräten werden die Enhacementlayer 
ignoriert. \parencite[144]{Richardson.2008} 

H264-SVC-fähige Geräte dekodieren sowohl den Baselayer als auch den Enhancementlayer. Die einzelnen Enhancementlayer 
erhöhen mit jeder Iteration des Enhancementlayers die Qualität in Bezug auf Auflösung, Bitrate oder Framerate.
Bei der Dekodierung wird aus dem im Enhancementlayer gespeicherten Deltawerten bzw Differenzen zum Baselayer  
und dem Baselayer selber das Videobild rekonstruiert \parencite[144]{Richardson.2008}.

\subsection {Spatiale Skalierbarkeit} 
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.95\textwidth]{bilder/base2enhancementlayer.jpg}
	%\caption[base2enhancementlayers]{Enkodierung von Baslayer und Enhancementlayer 
	%\\ Eigene Abbildung nach \textit{H.264 and MPEG-4 Video Compression Seite, 2008, Seite 143}}
	\caption[Enkodierung von Baslayer und Enhancementlayer ]{Enkodierung von Baslayer und Enhancementlayer \\ 
	(Eigene Abbildung nach \parencite[143]{Richardson.2008})}
	\label{fig:base2enhancementlayers}
\end{figure}

Die Videoenkodierung der Spatialen- bzw. Auflösungs-Enhancementlayer erfolgt, indem zuerst für jedem Frame des 
Videoeingangssignal (Siehe Abbildung \ref{fig:base2enhancementlayers} a)
der Baselayer (Siehe Abbildung \ref{fig:base2enhancementlayers} b)
mit verringerter spatialer Qualität in H264/AVC enkodiert wird. 

Um die Differenzen des Baselayers zum Originalvideobild zu erhalten, wird der zuvor erzeugte Baselayer auf 
die ursprüngliche Größe skaliert (Siehe Abbildung \ref{fig:base2enhancementlayers} c) und die Differenz beider Bilder 
extrahiert (Siehe Abbildung \ref{fig:base2enhancementlayers} d). 
Aus diesem Differenzbild wird der erste Enhancementlayer enkodiert. Jeder weitere Enhancementlayer enkodier die 
Differenz zum vorherigen, hochskalierten Enhancementlayer.
\parencite[142ff]{Richardson.2008}

Die Dekodierung der mit H264-SVC enkodierten Videoströmen erfolgt, indem zuerst der Baselayer dekodiert wird 
(Siehe Abbildung \ref{fig:decode_base2enhancementlayers} a) und auf die ursprüngliche Auflösung hochskaliert wird
(Siehe Abbildung \ref{fig:decode_base2enhancementlayers} b). Anschließend werden die Enhancementlayer dekodiert 
(Siehe Abbildung \ref{fig:decode_base2enhancementlayers} c) und deren Bildinformationen zum bereits dekodierten
Baselayer hinzugefügt (Siehe Abbildung \ref{fig:decode_base2enhancementlayers} d), um das vollständigen Videobild
wiederherzustellen.\parencite[144]{Richardson.2008}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.95\textwidth]{bilder/decode_base2enhancementlayer.jpg}
	%\caption[base2enhancementlayers]{Dekodierung von spatialen Enhancementlayern
	%\\ Abbildung nach \textit{H.264 and MPEG-4 Video Compression Seite, 2008, Seite 143}}
	\caption[Dekodierung von spatialen Enhancementlayern]{Dekodierung von spatialen Enhancementlayern \\ 
	(Eigene Abbildung nach \parencite[143]{Richardson.2008})}
	\label{fig:decode_base2enhancementlayers}
\end{figure}

\subsection {Temporale Skalierbarkeit}
Zur Erzeugung verschiedenerer temporaler- bzw. zeitlicher- Enhancementlayer verwendet H264-SVC, die regulären in 
H264/AVC angewandten hierarischen B-und P Frames\parencite[3]{WienNeu.2007}. 
Zur Erzeugung der temporalen Enhancementlayer werden die einzelnen Frames der in höchster Framerate
vorliegende Videodatei auf die einzelnen Layer aufgeteilt. Die Länge der GOP ist dabei relativ frei wählbar. 
Der Baselayer enthält die Qualitätsstufe mit der geringens Framerate. 
In diesem wird nur ein geringer Teil der GOP gespeichert, beispielsweise I-Frame des ersten und letzten 
Frames der GOP und ein weiterer B-Frame.\parencite[144]{Richardson.2008}

Die Enhancementlayer kodieren sukzessive jeweils höhere Frameraten, indem sie auf die Frames der vorhergehenden 
Layer referenzieren. Im ersten Enhancementlayer werden weitere B-Frames des Originalvideodatei gespeichert.
Diese B-Frames referenzieren die I-Frames des Baselayers. Im nachfolgendem Enhancementlayer werden wiederum
weitere B-Frames des Originalvideodatei gespeichert. Damit referenziert dieser nun den Baselayer und den ersten
Enhancementlayer.\parencite[292]{Richy.2011}. 

Dieses Prinzip setzt sich bis zum letzten Baselayer fort. Bei der Dekodierung muß man daher alle Base-
und Enhancementlayer dekodieren, die dem gewünschten Enhancementlayer vorangehen.
\parencite[292]{Richy.2011}.    

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.95\textwidth]{bilder/svc_temporal.jpg}
	%\caption[base2enhancementlayers]{Temporale Skalierbarkeit
	%\\ Abbildung nach \textit{H.264 and MPEG-4 Video Compression Seite, 2008, Seite 143}}
	\caption[Temporale Skalierbarkeit]{Temporale Skalierbarkeit \parencite{Slivinski.2011}}
	\label{fig:svc temporal}
\end{figure}

% http://www.lifesize.com/videoconferencingspot.com/?p=1406

\subsection{Qualitative Skalierbarkeit}
H264-SVC enkodierte Videoströme mit qualitativer Skalierbarkeit besitzen mehrere Layer mit unterschiedlicher 
Bildqualitätsstufen. Diese Art der Skalierbarkeit wird auch als SNR- Skalierbarkeit bezeichnet.
Im Detail unterstützt H264-SVC drei unterschiedliche Implementierung für qualitative Skalierbarkeit.
\parencite[10]{Unanue.2011}

Die Basisimplementation ist das sogenante Coarse-grain Quality Scalable Coding (CGS). Dies kann als Sonderfall der 
spatialen Skalierbarkeit angesehen werden. Zuerst wird aus dem Videoeingangsignal der Baselayer mit geringer 
Qualität (z.\,B. Bitrate) generiert. Anschließend werden in nachfolgenden Enhancementlayern die Differenz 
zwischen dem I-Frame des Originalvideos und dem I- Frame des vorherigem Layers mit festen Bitraten kodiert
\parencite[4]{WienNeu.2007}. Innhalb der GOP werden nur die Referenzbilder im eigenen Layer kodiert. 
Referenzierungen zwischen den einzelnen Layern findet nichts statt.\parencite[8f]{SchwarzNeu.2007}. 
Skalierbarkeit wird in diesem Fall durch das komplette Entfernen einzelner Enhancementlayer erreicht, wodurch
allerdings die Flexibilität und Effizienz eingeschränkt ist. \parencite[2]{Gupta.2012}
Die Anzahl der Layer ist auf bestimmte Bitraten beschränkt und der Wechslen zwischen verschiedenen Layern ist
nur an an den GOP-Grenzen bzw. I-Frames möglich \parencite[7]{Shahid.2010}.

Eine verbesserte Implementation ist das sogannte Medium Grain Scalability (MGS), bei welchem innerhalb der GOP 
des Baselayers auch auf die Referenzframes des nachfolgendem Enhancementlayers zugegriffen werden kann. 
Dies erlaubt eine flexiblere Skalierbarkeit, erhöht aber die Fehleranfälligkeit.
\parencite[8]{Unanue.2011}

Die dritte Implementation ist die sogannte Fine Grain Scalability (FGS). Diese Art der Skalierung baut auf 
CGS auf und verfeinert diese Methode. Die Bitrate der einzelnen Layer wird dynamisch an die aktuell verfügbare
Bandbreite angepasst. Referenzierung einzelner Frames innerhalb der GOP wird nur im Baselayer durchgeführt.
\parencite[8]{Unanue.2011}

\section{H264-SVC mit MPEG DASH}
MPEG DASH (Dynamic Adaptive Streaming over HTTP) definiert ein HTTP-basierendes Streamingverfahren.  
\parencite[1]{FraunhoferWS.2012}. Durch die Verwendung von HTTP gegenüber anderen Protokollen wie
RTP ergeben sich mehrere Vorteile. In vielen Einsatzbereichen (z.\,B. Unternehmen) sind aus Sicherheitsgründen 
nur absolut notwenige Ports geöffnet, unter anderem HTTP auf Port 80. Zusätzlich entfällt die 
Auslieferung der Streaminginhalte über einen dedizierten Streamingserver. Stattdessen können reguläre Webserver
zur Ausliferung benutzt werden. Dadurch sinken für Streaminganbieter die Kosten für die Auslieferung des Contents
\parencite[3]{Yago.2011}.

Zur Auslieferung des Videocontents spezifiziert MPEG DASH ein XML-Dokument, das sogenannte Media Presentation Document 
(MPD), welches die einzelnen Segmente der Videodatei definiert und referenziert. Auf diese einzelnen Segmente kann 
per HTTP-GET zugegriffen werden \parencite[1]{FraunhoferWS.2012}.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.99\textwidth]{bilder/mpegDashSVC.jpg}
	%\caption[base2enhancementlayers]{MPG DASH 
	%\\ Abbildung nach \textit{H.264 and MPEG-4 Video Compression Seite, 2008, Seite 143}}	
	\caption[Media Presentation Document und H264-SVC]{Media Presentation Document und H264-SVC \\ 
	(Eigene Abbildung nach \parencite{FraunhoferWS.2012})}	
	\label{fig:mpegdash}
\end{figure}

Jedes in der MPD definierte Segment besteht aus einer Abfolge von Zeiteinheiten bzw. Periods, deren Intervall beliebig 
gewählt werden kann. In meisten Fällen beträgt dieses Intervall 2 bis 10 Sekunden. Jede Zeiteinheit besteht aus einem 
oder mehreren Adaptationsets z.\,B. jeweils für die jeweiligen Audiodaten, Videodaten oder Untertitel.
Jedes Adaptionset besteht wiederum aus verschiedenen Representationen für unterschiedliche Endgeräte (z.\,B. 
Unterscheidung via Bitrate oder Auflösung). In den einzelnen Representationen werden jeweils wiederum einzelne 
Segmente gespeichert, die durch eine eigene URL per HTTP-GET aufrufbar 
sind \parencite[1]{FraunhoferWS.2012}.

Durch diese modulare Struktur eignet sich MPEG DASH sehr gut zur Auslieferung von H264-SVC enkodierte Videoströmen. 
Die einzelnen Baselayer und Enhancementlayer können jeweils in eigene Representationen gespeichert werden
und innheralb der Adaptationsets referenziert werden. \parencite[14]{Yago.2011}.

Die Zuordnung der einzelnen Layer erfolgt dabei über zusätzliche Attribute in der MPD-Datei. Jede Representation, 
die ein Layer definiert, erhält ein ID-Attribut und ein DependencyId -Attribut. Dieses gibt an auf welche vorherige
Layer, sich dieser Layer bezieht und welche zuvor dekodiert werden müssen\parencite[16]{Yago.2011}.


\chapter{H264-SVC Hands-on}

\section{SVC-Referenzcodec}
Zur Evaluation der H264-SVC-Erweiterung wird der \textit{JSVM-COdec} (Joint Scalable Video Model) verwendet. 
Dieser ist der Referenzcodec des Joint Video Team (JVT), der ISO/IEC Moving Pictures Experts Group (MPEG) und 
der ITU-T Video Coding Experts Group (VCEG). Neben dem Encoder und Decder sind im JMVC-Softwarepakets auch 
Multiplexer und Testprogramme enthalten.

Das JMVC-Softwarepaket ist im Quelltext frei verfügbar. Der Quelltext für das Softwarepaket ist in C++ 
geschrieben und auf dem Webspace der Rheinisch-Westfälische Technische Hochschule Aachen (RWTH-Aachen) 
in einem CVS-Repository gehostet. Um dieses Abzurufen ist eine geeigneten CVS-Software wie Tortoise 
notwendig 

Die hier vewendete Version des Encoder ist die Version XXXX.

\begin{table}[htb]
\centering	
	\begin{tabular}{p{0.3\textwidth} p{0.3\textwidth} }
			\textbf{Autentifzierungstyp}& pserver \\
			\textbf{Hostadresse}& garcon.ient.rwth-aachen.de \\
			\textbf{CVS-Pfad}& /cvs/jvt \\
			\textbf{Benutzername}& jvtuser \\
			\textbf{Passwort}& jvt.Amd.2 \\
			\textbf{Modulname}&jsvm oder jsvm_red \\			
	\end{tabular}
	\caption{Zugangsdaten für das CVS-Repository der RWTH Aachen}
	\label{tab:TabelleCVS}
\end{table}

\subsection{Installation}
Nach dem lokalen Download aus dem CVS der RWTH-Aachen befindet sich der Quellcode mit allen benötigen 
Klassen und Bibliotheken im Unterordner \textit{jsvm/h264Extension/build/windows}. Zur Vereinfachung der 
Installation werden bereits vorkonfigurierte Projektmappen für Microsoft Visual Studio mitgeliefert. 
Nach Öffnen der Projektmappe in Visual Studio wird mit der Menüoption \textit{Create Batch/ 
Projektmappe erstellen} der Quellcode kompiliert.
Nach erfolgreichem Kompilieren sind alle ausführbaren Progamme im Unterordner \textit{jmvc/bin} zu 
finden.

Die im Test verwendeten Beispielvideodatein können als YUV-Dateien unter folgender URL downgeloadet werden:
\url{ftp://ftp.tnt.uni-hannover.de/pub/svc/testsequences/}

\subsection{Vorbereitung}
Der Referenzeencoder benötig für die Erstellung eines beliebigen skalierbaren Videostroms mehrere Versionen eines 
Quellvideos, das sich in Bezug auf Bildgröße, Framerate oder Bildqualität unterscheidet. Da diese unterschiedlichen
Versionen meist nicht vorliegen, besitzt der Referenzencoder bereits ein Unterprogramm zur Erstellung dieser
Videodateien. Das Programm heißt "`DownConvertStatic"' und wird mit folgender Syntax aufgerufen:

\begin{lstlisting}[label=DownConvertStatic,caption=Aufruf DownConvertStatic ,captionpos=b,breaklines=true,mathescape=true]
$\textbf{DownConvertStatic}$ 352 2 88 bus_cif.yuv 176 144 bus_qcif.yuv
\end{lstlisting}

Anschließend befinden neben der urpsünglichen YUV-Datei weitere Videodatein mit unterschiedlicher spatialer 
Qualität im  Verzeichniss.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{bilder/encoder/downConvert.PNG}
	\caption[Referenzencoder]{Referenzencoder}
	\label{fig:referenzencoder}
\end{figure}

\subsection{Enkodieren}
Zur Erzeugung eines skalierbaren Videostroms werden mehrere Konfigurationsdateien benötigt. Einerseits wird
eine allgemeine Konfigurationsdatei benötigt, in welcher die Einegnschaften der Quelldatei definiert werden.
Dazu zählen u.a. der Dateiname, die Auflösung, die zu enkodierenden Frames und die Größe der Group of Pictures.
Zusätzlich werden die layerspezifischen Konfigurationsdateien in dieser Konfigurationsdatei verlinkt.   

\begin{lstlisting}[label=ConfigEncoder,caption=Konfigurationsdatei des Encoders ,captionpos=b,breaklines=true,mathescape=true]

# JSVM Main Configuration File

OutputFile          test.264   # Bitstream file
FrameRate           30.0       # Maximum frame rate [Hz]
FramesToBeEncoded   150        # Number of frames
GOPSize             16         # GOP Size (at maximum frame rate)
BaseLayerMode       2          # Base layer mode
SearchMode          4          # Search mode
SearchRange         32         # Search range (Full Pel)
NumLayers           1          # Number of layers
LayerCfg            layer0.cfg # Layer configuration file
\end{lstlisting}

Weiterhin wird für jeden Layer eine zustzliche Konfigurationsdatei benötigt. In dieser spezifischen Datei werden 
u.a. der Dateiname der herunterskalierten Quelldatei, die Auflösug, Bitrate und die Framerate für diesen Layer 
definiert.

\begin{lstlisting}[label=ConfigLayer,caption=Konfigurationsdatei für einen Layer ,captionpos=b,breaklines=true,mathescape=true]

# JSVM Layer Configuration File

InputFile           BUS_CIF30.yuv  # Input  file
SourceWidth         352            # Input  frame width
SourceHeight        288            # Input  frame height
FrameRateIn         30             # Input  frame rate [Hz]
FrameRateOut        30             # Output frame rate [Hz]
InterLayerPred      2              # Inter-layer Pred.

\end{lstlisting}

Der Encoder \textbf{H264AVCEncoderLibTestStatic} wird über folgenden Befehl aufgerufen:

\begin{lstlisting}[label=Encoder,caption=Aufruf H264AVCEncoderLibTestStatic ,captionpos=b,breaklines=true,mathescape=true]
$\textbf{H264AVCEncoderLibTestStatic}$ –pf Konfigurationsdatei.cfg
\end{lstlisting}

Wenn die Enkodierung erfolgreich abgeschlossen ist, gibt er Encoder anschließend eine Meldung über die 
enkodierten und eingebetteten Layer aus. Die entsprechende enkodierte Videodatei befindet sich Im Verzeichniss 
des Enkoders unde trägt die Dateiendung .h264.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\textwidth]{bilder/encoder/encoderDone.PNG}
	\caption[Referenzencoder]{Referenzencoder}
	\label{fig:referenzencoder}
\end{figure}

\subsection{Dekodieren}  

\subsection{Bitstreams extrahieren}

%\paragraph{Enkoder}
%COnfig File
%SIngle LAyer
%Multi Layer

\subsection{Evaluation}
Inputfile muss unkomprimiertes YUV-Video sein --> Großes Files, Umkodieren notwendig
Nicht Geschwindigkeitsoptimiert/langsam/nur Singlecore (10s pro Frames)

\section{Kommerzielle Anbieter/Verwendung}

\subsection{Google Video}

\subsection{Mainconcept SVC}

%\section{Vorteile und Nachteile}
\chapter{Konklusion}
Die Verwendung von H264-SVC zur Distribution von Videoformaten bringt entscheidende Vorteile gegenüber 
herkömmlichen Verfahren, wie das Simulcastverfahren, also der Übertragung mehrer unabhängiger Videoströme 
in unterschiedlicher Qualität.

Im praktischen Anwendungsfall, dass gleichzeitig ein Videostrom an mobile Endgeräte, HD-Ready-Geräte und 
Geräte mit FullHD- Auflösung übertragen wird, braucht man durch die Nutzung von spatialen Enhancemenlayern 
keine dedizierten Videodateien für bestimmte Endgeräteklassen zu enkodieren und archivieren. 
Stattdessen wird ein Baselayer für alle Endgeräte enkodiert, der nur den Videostrom in SD-Auflösung enthält. 
Mobile Endgeräte dekodieren ausschließlich diesen Baselayer. Geräte mit HD-Ready-Auflösung dekodieren
zusätzlich den ersten Enhancementlayer in HD-Ready-Auflösung, der die Differenz zum Baselayer enthält.
Für Geräte mit FullHD-Display werden alle Enhancementlayer dekodiert. Daduch ist man sehr flexibel in Hinblick auf die 
entsprechenden Endgeräte und kann sehr einfach für weitere Geräte zusätzliche Enhancementlayer definieren.
\parencite[602]{Sader.2008}

Nachteilig an H264-SVC ist allerdings, dass es derzeit keine große Anzahl an kommerziellen Anbieter von SVC-basierten 
Applikationen gibt, die den kompletten Worklfow von Enkodierung, Übertragung und Dekodierung abbilden. 
Dies ist daran zu begründen, dass in der Regel nicht SVC-basierende Lösungen einfacher zu implementieren sind, da
sie auf etablierten Techniken wie reguläres H264/AVC basieren und dadurch mittlerweile einen gewissen Reifegrad 
erreicht haben. 

Allerdings schreitet die Diversifikation der Endgeräte und Übertragungsmedien weiter voran, sodass auf lange Sicht
durch Simulcast nicht alle Geräteklassen, Qualitätsstufen und Übertragungsmedien abbildbar sind und somit nicht 
für jedes Gerät die optimale Videoübertragung verfügbar ist. \\ 
Auf lange Sicht wird daher der Bedarf an skalierbaren Videocodecs weiter steigen und damit wird auch die Verbeitung 
von H264-SVC zunehmen \parencite[603]{Sader.2008}

\appendix
%\nocite{*}
\listoffigures
\pagenumbering{Alpha}
\printbibliography

\end{document}
